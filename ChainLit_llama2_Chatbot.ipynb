{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPpsXJAjxCrZPcL1kClLZmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdushie/chainlit_llm_chatbot/blob/main/ChainLit_llama2_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries"
      ],
      "metadata": {
        "id": "Wp4kd3eth9uj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skyAdhmEh7Ww",
        "outputId": "8b894a89-f657-4365-c3fb-b15814006a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chainlit\n",
            "  Downloading chainlit-1.0.502-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.15-py3-none-any.whl (814 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.5/814.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Collecting aiofiles<24.0.0,>=23.1.0 (from chainlit)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting asyncer<0.0.3,>=0.0.2 (from chainlit)\n",
            "  Downloading asyncer-0.0.2-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from chainlit) (8.1.7)\n",
            "Collecting dataclasses_json<0.6.0,>=0.5.7 (from chainlit)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting fastapi<0.111.0,>=0.110.1 (from chainlit)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-socketio<0.0.11,>=0.0.10 (from chainlit)\n",
            "  Downloading fastapi_socketio-0.0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from chainlit)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting httpx>=0.23.0 (from chainlit)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lazify<0.5.0,>=0.4.0 (from chainlit)\n",
            "  Downloading Lazify-0.4.0-py2.py3-none-any.whl (3.1 kB)\n",
            "Collecting literalai==0.0.500 (from chainlit)\n",
            "  Downloading literalai-0.0.500.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from chainlit) (1.6.0)\n",
            "Collecting packaging<24.0,>=23.1 (from chainlit)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from chainlit) (2.6.4)\n",
            "Collecting pyjwt<3.0.0,>=2.8.0 (from chainlit)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from chainlit)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting python-graphql-client<0.5.0,>=0.4.3 (from chainlit)\n",
            "  Downloading python_graphql_client-0.4.3-py3-none-any.whl (4.9 kB)\n",
            "Collecting python-multipart<0.0.10,>=0.0.9 (from chainlit)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from chainlit)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting syncer<3.0.0,>=2.0.3 (from chainlit)\n",
            "  Downloading syncer-2.0.3.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from chainlit) (2.0.1)\n",
            "Collecting uptrace<2.0.0,>=1.22.0 (from chainlit)\n",
            "  Downloading uptrace-1.24.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting uvicorn<0.26.0,>=0.25.0 (from chainlit)\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles<0.21.0,>=0.20.0 (from chainlit)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chevron>=0.14.0 (from literalai==0.0.500->chainlit)\n",
            "  Downloading chevron-0.14.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.41 (from langchain)\n",
            "  Downloading langchain_core-0.1.41-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.45-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.10.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from asyncer<0.0.3,>=0.0.2->chainlit) (3.7.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json<0.6.0,>=0.5.7->chainlit)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses_json<0.6.0,>=0.5.7->chainlit)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting python-socketio>=4.6.0 (from fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n",
            "  Downloading python_socketio-5.11.2-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.23.0->chainlit)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->chainlit) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.23.0->chainlit)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->chainlit) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->chainlit) (2.16.3)\n",
            "Collecting websockets>=5.0 (from python-graphql-client<0.5.0,>=0.4.3->chainlit)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Collecting opentelemetry-api~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_exporter_otlp-1.24.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting opentelemetry-instrumentation~=0.45b0 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-sdk~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.4.0->asyncer<0.0.3,>=0.0.2->chainlit) (1.2.0)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.24.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.24.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (1.62.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation~=0.45b0->uptrace<2.0.0,>=1.22.0->chainlit) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation~=0.45b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.14.1)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit) (0.23.1)\n",
            "Collecting python-engineio>=4.8.0 (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n",
            "  Downloading python_engineio-4.9.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json<0.6.0,>=0.5.7->chainlit)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (3.18.1)\n",
            "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n",
            "  Downloading simple_websocket-1.0.0-py3-none-any.whl (13 kB)\n",
            "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: literalai, syncer\n",
            "  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for literalai: filename=literalai-0.0.500-py3-none-any.whl size=46119 sha256=761666dda5b6ce84aa72ad64187760c3429fff34755e3b668f94a289e66f1f33\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/18/c7/61d2aba9698a110f1d08a294b72346e0710bd2216a5f5ed886\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syncer: filename=syncer-2.0.3-py2.py3-none-any.whl size=3436 sha256=bc630b3168c9df06087fe20364714ef59abe0f1e6370cfb2514cb22ec6410d95\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/e4/36/bcaad665bcc2b672888ff2df1a5a1dc638378d8765055313cd\n",
            "Successfully built literalai syncer\n",
            "Installing collected packages: syncer, lazify, filetype, faiss-gpu, chevron, websockets, python-multipart, python-dotenv, pypdf, pyngrok, pyjwt, packaging, orjson, opentelemetry-semantic-conventions, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, jsonpointer, importlib-metadata, h11, faiss-cpu, deprecated, aiofiles, wsproto, watchfiles, uvicorn, typing-inspect, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, jsonpatch, httpcore, asyncer, simple-websocket, python-graphql-client, opentelemetry-sdk, opentelemetry-instrumentation, nvidia-cusolver-cu12, langsmith, httpx, fastapi, dataclasses_json, ctransformers, python-engineio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, literalai, langchain-core, sentence-transformers, python-socketio, opentelemetry-exporter-otlp, langchain-text-splitters, langchain_community, uptrace, langchain, fastapi-socketio, chainlit\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed aiofiles-23.2.1 asyncer-0.0.2 chainlit-1.0.502 chevron-0.14.0 ctransformers-0.2.27 dataclasses_json-0.5.14 deprecated-1.2.14 faiss-cpu-1.8.0 faiss-gpu-1.7.2 fastapi-0.110.1 fastapi-socketio-0.0.10 filetype-1.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 importlib-metadata-7.0.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.15 langchain-core-0.1.41 langchain-text-splitters-0.0.1 langchain_community-0.0.32 langsmith-0.1.45 lazify-0.4.0 literalai-0.0.500 marshmallow-3.21.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 orjson-3.10.0 packaging-23.2 pyjwt-2.8.0 pyngrok-7.1.6 pypdf-4.2.0 python-dotenv-1.0.1 python-engineio-4.9.0 python-graphql-client-0.4.3 python-multipart-0.0.9 python-socketio-5.11.2 sentence-transformers-2.6.1 simple-websocket-1.0.0 starlette-0.37.2 syncer-2.0.3 typing-inspect-0.9.0 uptrace-1.24.0 uvicorn-0.25.0 watchfiles-0.20.0 websockets-12.0 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install chainlit langchain langchain_community pypdf sentence-transformers faiss-gpu faiss-cpu ctransformers pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive to hold LLAMA2 model"
      ],
      "metadata": {
        "id": "4L5kojg6zDnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnaFpsngznPz",
        "outputId": "0971c0ef-191a-4ed4-a0be-07659a55d292"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download llama2-7b model"
      ],
      "metadata": {
        "id": "c1hsOpnjkNl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin?download=true"
      ],
      "metadata": {
        "id": "okmVUTWUkM0x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy llama2 model to my Google Drive"
      ],
      "metadata": {
        "id": "vXtoG8eT1qDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cp /content/llama-2-7b-chat.ggmlv3.q8_0.bin /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "Q0yRY8iF1wX0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cp -r /content/data_interview/ /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "olbHpIBIHd6f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy llama2 model from Google Drive to content"
      ],
      "metadata": {
        "id": "IhRc9MW6DNTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/drive/MyDrive/llama-2-7b-chat.ggmlv3.q8_0.bin /content/"
      ],
      "metadata": {
        "id": "zEgw-6gdDXzd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy PDF files (for training) contained in data_interview to /content and rename folder to data"
      ],
      "metadata": {
        "id": "yOwl90JRDsxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cp -r /content/drive/MyDrive/data_interview /content/\n",
        "%mv /content/data_interview /content/data"
      ],
      "metadata": {
        "id": "oHb77kkQD-Ow"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy my public folder fom my Google Drive to /content and rename it from public_llm to public"
      ],
      "metadata": {
        "id": "3Wv_GFmhEexD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cp -r /content/drive/MyDrive/public_llm /content/\n",
        "%mv /content/public_llm /content/public"
      ],
      "metadata": {
        "id": "5OX93yoqEfh5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and write model.py to file"
      ],
      "metadata": {
        "id": "BqpgYdYtFAAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "import time\n",
        "import chainlit as cl\n",
        "\n",
        "DB_FAISS_PATH = 'vectorstore/'\n",
        "\n",
        "custom_prompt_template = \"\"\"Use the following information to answer the user's question.\n",
        "If you don't know the answer, let the user know that.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Return a useful answer.\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "def set_custom_prompt():\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def retrieval_qa_chain(llm, prompt, db):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type='stuff',\n",
        "                                       retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                       )\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "def load_llm():\n",
        "    # Load the locally downloaded model here\n",
        "    llm = CTransformers(\n",
        "        model = \"TheBloke/Llama-2-7B-Chat-GGML\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens = 200,\n",
        "        temperature = 0.5\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "\n",
        "def qa_bot():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                       model_kwargs={'device': 'cuda'})\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    llm = load_llm()\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
        "    return qa\n",
        "\n",
        "def final_result(query):\n",
        "    qa_result = qa_bot()\n",
        "    response = qa_result({'query': query})\n",
        "    return response\n",
        "\n",
        "############\n",
        "# Chainlit #\n",
        "############\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    chain = qa_bot()\n",
        "    msg = cl.Message(content=\"Getting things ready...\")\n",
        "    await msg.send()\n",
        "    msg.content = \"Hi, Welcome to the Ace Interview Bot. I know a lot than you can imagine about interviews.  How can I be of assistance today?\"\n",
        "    await msg.update()\n",
        "\n",
        "    cl.user_session.set(\"chain\", chain)\n",
        "\n",
        "\n",
        "def sync_func():\n",
        "    time.sleep(5)\n",
        "    return \"Hello!\"\n",
        "\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    chain = cl.user_session.get(\"chain\")\n",
        "    cb = cl.AsyncLangchainCallbackHandler(\n",
        "        stream_final_answer=True, answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
        "    )\n",
        "    cb.answer_reached = True\n",
        "    res = await chain.acall(message.content, callbacks=[cb])\n",
        "    answer = res[\"result\"]\n",
        "    #sources = res[\"source_documents\"]\n",
        "\n",
        "    #if sources:\n",
        "         #answer += f\"\\nSources:\" + str(sources)\n",
        "    #else:\n",
        "         #answer += \"\\nNo sources found\"\n",
        "\n",
        "    #await cl.Message(content=answer).send()\n",
        "    await cl.Message(\n",
        "        content=answer,\n",
        "    ).send()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz1N0HX8iSod",
        "outputId": "23353d6a-4ee4-47e5-b69f-d7da7f9f646c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ingest.py\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "\n",
        "DATA_PATH = \"data/\"\n",
        "FAISS_PATH = \"vectorstore/\"\n",
        "\n",
        "\n",
        "def load_vector_db():\n",
        "    loader = DirectoryLoader(DATA_PATH,glob=\"*.pdf\",loader_cls = PyPDFLoader)\n",
        "    data = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(data)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2', model_kwargs = {'device':'cuda'})\n",
        "\n",
        "    db = FAISS.from_documents(texts,embeddings)\n",
        "    db.save_local(FAISS_PATH)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    load_vector_db()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU5uMPQbjc5c",
        "outputId": "cb5db7ca-20ce-4a31-d8bd-0f339d36340a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ingest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run python ingest.py"
      ],
      "metadata": {
        "id": "liGeJjwrrsPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ingest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TPxyfZom_vp",
        "outputId": "0d1f4a52-4918-4bf1-89e0-ea555b6d4b64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rmodules.json:   0% 0.00/349 [00:00<?, ?B/s]\rmodules.json: 100% 349/349 [00:00<00:00, 2.04MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 411kB/s]\n",
            "README.md: 100% 10.7k/10.7k [00:00<00:00, 43.1MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 330kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 4.01MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:00<00:00, 270MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 2.31MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 901kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.53MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 734kB/s]\n",
            "1_Pooling/config.json: 100% 190/190 [00:00<00:00, 1.24MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run python model.py"
      ],
      "metadata": {
        "id": "Syh92p__r1Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_epDuBQnG7l",
        "outputId": "ccbf39fb-7002-4a9a-e74d-edadb9a37e53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 02:20:25 - Created default config file at /content/.chainlit/config.toml\n",
            "2024-04-11 02:20:25 - Created default translation directory at /content/.chainlit/translations\n",
            "2024-04-11 02:20:25 - Created default translation file at /content/.chainlit/translations/en-US.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run model.py"
      ],
      "metadata": {
        "id": "UGXUCSvvvpPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run model.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "TTBlecRhvplA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy customized chainlit.md file from MyDrive to content"
      ],
      "metadata": {
        "id": "P1n9OxFd5exQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/chainlit.md /content"
      ],
      "metadata": {
        "id": "4lBgD11u5JUX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrok setup to enable public access to colab project (Chainlit LLM App)"
      ],
      "metadata": {
        "id": "mi1_E4RsuElx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ngrok authtoken - Go to ngrok and create an account\n",
        "!ngrok config add-authtoken 2esw16uUBmWdB4r3HtL5B6coxID_3zcxjYgKcQYUqiR6TkCkJ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBYi1VaEuArx",
        "outputId": "a5f1b15c-c773-4c52-f5a7-4c158d0fd7ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Ngrok tunnel"
      ],
      "metadata": {
        "id": "2MUbyXpzuzz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWx5ym93u_tf",
        "outputId": "50827b85-18f3-4c03-de31-090844ea3b58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://37c0-34-16-160-106.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ngrok.kill()"
      ],
      "metadata": {
        "id": "Bp9zS1novGz7"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}